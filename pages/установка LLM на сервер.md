- куда ставить? на CPU или GPU?
-
- Зависит от того, **какую именно модель** вы хотите развернуть и как вы планируете её использовать.
  
  Смотрите в целом:
- ### 1. Когда хватает CPU
  
  Подходят CPU‑серверы, если:
- Нужны **небольшие локальные LLM** (например, 1–7B параметров в quantized-формате, типа ggml/gguf);
- Нагрузка небольшая: тесты, разработка, один‑два пользователя;
- Важна экономия бюджета, а не максимальная скорость.
  
  Плюсы:
- Дешевле конфигурации.
- Простая настройка (любой наш облачный сервер подойдёт).
  
  Минусы:
- **Ответы модели будут заметно медленнее**, особенно у средних и больших моделей.
- При нескольких одновременных запросах модель начинает «тормозить».
  
  Для CPU‑варианта обычно берут конфигурации с большим количеством ядер и достаточным объёмом RAM (например, 8–16 ГБ и выше, в зависимости от модели).
  
  ---
- ### 2. Когда нужен GPU
  
  GPU нужен, если:
- Модель **средняя или большая** (7B+, 13B, 70B и т.п.);
- Важна **скорость ответа** (онлайн‑чат с пользователями, интеграция в продукт);
- Планируется **несколько одновременных пользователей** или фоновые задачи (обработка текстов, генерация).
  
  В Timeweb.Cloud есть облачные сервера с GPU:
- Сейчас мы предоставляем **NVIDIA Tesla T4 16 GB**.
- При создании сервера в панели:
  1. На шаге с конфигурацией перейдите во вкладку **«Произвольная»**.
  2. Выберите тип **GPU** и укажите количество видеокарт (от 1 до 4 — зависит от наличия).
  3. Минимальная конфигурация по CPU и RAM под конкретное число GPU будет подобрана автоматически (по правилам: на одну видеокарту требуется минимум 1 ядро CPU и 2 ГБ RAM).
  
  Если нужна другая видеокарта или особая конфигурация — это решают через поддержку.
  
  ---
- ### 3. Что выбрать на практике
  
  Если обобщить:
- **Прототип, тест, маленькие модели, офлайн-скрипты → CPU**  
  Подойдёт обычный облачный сервер без GPU, главное — достаточно RAM под модель.
- **Продуктовая нагрузка, большие модели, быстрый отклик, много запросов → GPU**  
  Тогда при создании сервера выбирайте конфигурацию с GPU.
  
  ---
  
  Если напишете:
- какую именно модель (название, размер),
- сколько планируется пользователей/запросов,
- и какой бюджет примерно,
  
  я подскажу, какой тип сервера в Timeweb.Cloud подобрать и на что обратить внимание при выборе конфигурации.  
  
  Если потребуется нестандартная конфигурация GPU, можно обратиться в поддержку:  
  **Связаться с поддержкой можно обращением из [личного кабинета](https://timeweb.cloud/my/support) или письмом на почту info@timeview.cloud.**
-
- Вот сравнительная таблица моделей, отсортированных от меньших системных требований к большим:
- | Модель | Системные требования и особенности |
  | --- | --- |
- | SmolLM2-360M | Подходит для ноутбука 2008 года с 1 ГБ оперативной памяти, процессором Celeron и 32-битной операционной системой. Также хорошо работает на старых Raspberry Pi. | 1 2 3
- | Gemma-2-2B | Подходит для быстрого перевода. Более ресурсоёмкая, чем SmolLM2-360M, но менее требовательна, чем модели с большим количеством параметров. | 4
- | Phi3.5-Mini (4B) | Позволяет работать с документами объёмом до 40 тысяч токенов. Требует больше ресурсов, чем Gemma-2-2B. | 5
- | Qwen2.5-Coder-7B | Лучше выбрать эту модель, а не DeepSeek, если у вас ограничена память. Не является MoE-моделью, поэтому немного медленнее DeepSeek-Coder-V2-Lite. | 6 7 8
- | DeepSeek-Coder-V2-Lite (16B) | Использует 2B параметров при выводе, что делает её быстрой, но требует больше ресурсов, чем модели меньшего размера. | 9
- | Mistral-Nemo-2407 (12B) | Универсальная модель, обучена на длину контекста 128K, но её эффективная длина контекста ближе к 16K. Требует значительных ресурсов. | 10 11
- | Qwen2.5-14B | Впечатляюще превосходит ожидания на любом масштабе, но требует ещё больше ресурсов, чем Mistral-Nemo-2407. | 12 13
- | Mixtral-8x7B (48B) | Модель Mixture of Experts (MoE), при выводе использует только 13B параметров одновременно. Подходит для инференса на CPU на компьютере с не менее чем 32 ГБ оперативной памяти. | 14 15 16
- | Llama-3.1-70B и Llama-3.1-Nemotron-70B | Очень требовательны к ресурсам, автор статьи не может запустить их на своём оборудовании, использует удалённый доступ. | 17